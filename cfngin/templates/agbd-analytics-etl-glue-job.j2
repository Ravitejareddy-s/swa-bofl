{%- macro format_name(name) -%}
  {{ name | replace('-','') | replace('_','') | truncate(92, true, '') }}
{%- endmacro -%}

{%- macro hyphen_to_underscore(str) -%}
  {{ str | replace('-','_') | truncate(92, true, '') }}
{%- endmacro -%}

{% for job in variables.vModuleConfig['vEtlJobs']['jobs'] %}
  #-------------------------------------------------------------------------------
  # Glue ETL job for {{job.name}}
  #
  # Glue spark job with script in S3
  #-------------------------------------------------------------------------------
  rGlueJob{{format_name(job.name)}}:
    Type: AWS::Glue::Job
    Properties:
      Name: !Join ['-', [!Ref pDepartment, !Ref pNamespace, {{job.name}}]]
      Role: !Ref pIamRole
      Description: 'Glue ETL job'
      GlueVersion: '4.0'
{% if job['numberOfWorkers'] is defined %}
      NumberOfWorkers: {{job.numberOfWorkers}}
{% else %}
      NumberOfWorkers: 5
{% endif %}
{% if job['WorkerType'] is defined %}
      WorkerType: {{job.WorkerType}}
{% else %}
      WorkerType: 'G.2X'
{% endif %}
{% if job['MaxConcurrentRuns'] is defined %}
      ExecutionProperty:
        MaxConcurrentRuns: {{job.MaxConcurrentRuns}}
{% else %}
      ExecutionProperty:
        MaxConcurrentRuns: 1
{% endif %}
{% if job['Timeout'] is defined %}
      Timeout: {{job.Timeout}}
{% else %}
      Timeout: 200
{% endif %}
      Command:
        Name: glueetl
        PythonVersion: "3"
{% if job['ScriptLocation'] is defined %}
        ScriptLocation:  !Sub "s3://${pS3ScriptBucket}/${pGlueScriptKeyPrefix}/src/{{job.ScriptLocation}}.py"
{% else %}
        ScriptLocation:  !Sub "s3://${pS3ScriptBucket}/${pGlueScriptKeyPrefix}/src/{{job.name}}.py"
{% endif %}
      DefaultArguments:
        "--TempDir":  !Sub "s3://${pS3ScriptBucket}/${pGlueScriptKeyPrefix}/src/tmp/{{job.name}}"
{% if job['pyFiles'] is defined %}
{% set s3_list = [] %}
{% for value in job['pyFiles'].keys() %}
    {{- s3_list.append("s3://${pS3ScriptBucket}/${pGlueScriptKeyPrefix}/src/"+value)|default("", True) -}}
{% endfor %}
        "--extra-py-files": !Sub "{{ s3_list|join(',') }}"
{% endif %}
{% if job['extra-jars'] is defined %}
{% set s3_list = [] %}
{% for value in job['extra-jars'].keys() %}
    {{- s3_list.append("s3://${pS3ScriptBucket}/${pGlueScriptKeyPrefix}/src/jars/"+value)|default("", True) -}}
{% endfor %}
        "--extra-jars": !Sub "{{ s3_list|join(',') }}"
{% endif %}
        "--enable-continuous-cloudwatch-log": true
        "--enable-auto-scaling": true
        "--enable-job-insights": true
        "--enable-metrics": true
        "--datalake-formats": 'hudi,delta,iceberg'
        "--enable-glue-datacatalog": true
        "--conf": "spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.hive.convertMetastoreParquet=false"
        "--base_s3_path": !Sub "s3://${pLakeFormationBucket}"
        '--enable-spark-ui': 'true'
        '--spark-event-logs-path': !Sub "s3://${pS3ScriptBucket}/${pGlueScriptKeyPrefix}/src/spark_logs/"
        '--enable-observability-metrics': 'true'
{% if job['otherLibFiles'] is defined %}
{% set s3_list = [] %}
{% for value in job['otherLibFiles'].keys() %}
    {{- s3_list.append("s3://${pS3ScriptBucket}/${pGlueScriptKeyPrefix}/src/"+value)|default("", True) -}}
{% endfor %}
        "--extra-files": !Sub "{{ s3_list|join(',') }}"
{% endif %}
{% if job['glueArguments']['configs'] is not defined %}
        "--configs": "configs"
{% endif %}

{% for key, val in job['glueArguments'].items()  %}
{% if key == 'bookmark' %}
        "--bookmark": !Sub "s3://${pLakeFormationBucket}/{{val}}"
{% else %}
        "--{{key}}": {{val}}
{% endif %}
{% endfor %}
{% endfor %}
